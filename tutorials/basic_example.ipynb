{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Example\n",
    "\n",
    "Here is a basic example of running a `VerificationSuite` with a couple `checks` and then filtering them based on their results. \n",
    "\n",
    "We'll start by creating a Spark session and a small sample dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ec2-user/anaconda3/envs/pydeequ-test/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/pydeequ-test/lib/python3.10/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ec5a5cf2-250a-4801-a8cf-2b280acac0b2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;1.2.2-spark-3.0 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.1 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "downloading https://repo1.maven.org/maven2/com/amazon/deequ/deequ/1.2.2-spark-3.0/deequ-1.2.2-spark-3.0.jar ...\n",
      "\t[SUCCESSFUL ] com.amazon.deequ#deequ;1.2.2-spark-3.0!deequ.jar (81ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze_2.12/0.13.2/breeze_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze_2.12;0.13.2!breeze_2.12.jar (462ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze-macros_2.12/0.13.2/breeze-macros_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze-macros_2.12;0.13.2!breeze-macros_2.12.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...\n",
      "\t[SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar ...\n",
      "\t[SUCCESSFUL ] net.sf.opencsv#opencsv;2.3!opencsv.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.github.rwl#jtransforms;2.4.0!jtransforms.jar (38ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.2!commons-math3.jar (81ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire_2.12/0.13.0/spire_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire_2.12;0.13.0!spire_2.12.jar (159ms)\n",
      "downloading https://repo1.maven.org/maven2/com/chuusai/shapeless_2.12/2.3.2/shapeless_2.12-2.3.2.jar ...\n",
      "\t[SUCCESSFUL ] com.chuusai#shapeless_2.12;2.3.2!shapeless_2.12.jar(bundle) (30ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.5!slf4j-api.jar (5ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.1/scala-reflect-2.12.1.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.1!scala-reflect.jar (45ms)\n",
      "downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] junit#junit;4.8.2!junit.jar (7ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire-macros_2.12/0.13.0/spire-macros_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire-macros_2.12;0.13.0!spire-macros_2.12.jar (4ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/machinist_2.12/0.6.1/machinist_2.12-0.6.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#machinist_2.12;0.6.1!machinist_2.12.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/macro-compat_2.12/1.1.1/macro-compat_2.12-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#macro-compat_2.12;1.1.1!macro-compat_2.12.jar (4ms)\n",
      ":: resolution report :: resolve 2921ms :: artifacts dl 974ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;1.2.2-spark-3.0 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.1 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   15  |   15  |   1   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/sonatype/oss/oss-parent/7/oss-parent-7.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/github/fommil/netlib/parent/1.1/parent-1.1.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/13/apache-13.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/commons/commons-parent/28/commons-parent-28.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/slf4j/slf4j-parent/1.7.5/slf4j-parent-1.7.5.jar\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ec5a5cf2-250a-4801-a8cf-2b280acac0b2\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (32953kB/56ms)\n",
      "22/08/16 02:36:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pydeequ\n",
    "\n",
    "# import sagemaker_pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# classpath = \":\".join(sagemaker_pyspark.classpath_jars()) # aws-specific jars\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "#     .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sparkContext.parallelize([\n",
    "    Row(a=\"foo\", b=1, c=5),\n",
    "    Row(a=\"bar\", b=2, c=6),\n",
    "    Row(a=\"baz\", b=3, c=None)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a='foo', b=1, c=5), Row(a='bar', b=2, c=6), Row(a='baz', b=3, c=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be importing the necessary `PyDeequ` modules for running a VerificationSuite with Checks. We will be checking the following: \n",
    "\n",
    "- does `df` have a size of at least 3? \n",
    "- does the `b` column have a minimum value of 0? \n",
    "- is the `c` column complete? \n",
    "- is the `a` column unique? \n",
    "- are the values of `a` column contained in \"foo\", \"bar\", and \"baz\"? \n",
    "- are the values in `b` colum non-negative? \n",
    "\n",
    "Once these checks are run, we'll display out the dataframe to see the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|           check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Integrity checks|      Error|       Error|SizeConstraint(Si...|          Failure|Value: 3 does not...|\n",
      "|Integrity checks|      Error|       Error|MinimumConstraint...|          Failure|Value: 1.0 does n...|\n",
      "|Integrity checks|      Error|       Error|CompletenessConst...|          Failure|Value: 0.66666666...|\n",
      "|Integrity checks|      Error|       Error|UniquenessConstra...|          Success|                    |\n",
      "|Integrity checks|      Error|       Error|ComplianceConstra...|          Success|                    |\n",
      "|Integrity checks|      Error|       Error|ComplianceConstra...|          Success|                    |\n",
      "+----------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "check = Check(spark, CheckLevel.Error, \"Integrity checks\")\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 4) \\\n",
    "        .hasMin(\"b\", lambda x: x == 0) \\\n",
    "        .isComplete(\"c\")  \\\n",
    "        .isUnique(\"a\")  \\\n",
    "        .isContainedIn(\"a\", [\"foo\", \"bar\", \"baz\"]) \\\n",
    "        .isNonNegative(\"b\")) \\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's nice to see those as a dataframe, but we noticed a couple **Failures** in the `constraint_status` column! Let's filter them by accessing the `checkResults` property of our run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found errors in the data, the following constraints were not satisfied:\n",
      "\tSizeConstraint(Size(None)) failed because: Value: 3 does not meet the constraint requirement!\n",
      "\tMinimumConstraint(Minimum(b,None)) failed because: Value: 1.0 does not meet the constraint requirement!\n",
      "\tCompletenessConstraint(Completeness(c,None)) failed because: Value: 0.6666666666666666 does not meet the constraint requirement!\n"
     ]
    }
   ],
   "source": [
    "if checkResult.status == \"Success\": \n",
    "    print('The data passed the test, everything is fine!')\n",
    "\n",
    "else:\n",
    "    print('We found errors in the data, the following constraints were not satisfied:')\n",
    "    \n",
    "    for check_json in checkResult.checkResults:\n",
    "        if check_json['constraint_status'] != \"Success\": \n",
    "            print(f\"\\t{check_json['constraint']} failed because: {check_json['constraint_message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkResult.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(check='Integrity checks', check_level='Error', check_status='Error', constraint='SizeConstraint(Size(None))', constraint_status='Failure', constraint_message='Value: 3 does not meet the constraint requirement!')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkResult_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pydeequ-test",
   "language": "python",
   "name": "conda_pydeequ-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
